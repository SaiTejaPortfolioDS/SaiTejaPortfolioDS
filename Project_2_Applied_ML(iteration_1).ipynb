{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiTejaPortfolioDS/SaiTejaPortfolioDS/blob/main/Project_2_Applied_ML(iteration_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JhbQfSY-ikH",
        "outputId": "94e1d88a-930f-4a5a-d468-eba66086f40f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Cleaning data...\n",
            "Preparing target...\n",
            "Encoding features...\n",
            "Found 6 categorical features\n",
            "Found 6 categorical features\n",
            "Engineering features...\n",
            "Splitting data...\n",
            "Training model...\n",
            "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
            "\n",
            "Validation Evaluation:\n",
            "AUCPR: 0.5848\n",
            "Threshold: 0.2765\n",
            "Confusion Matrix:\n",
            "[[86785 12117]\n",
            " [ 8234 12767]]\n",
            "\n",
            "Test Evaluation:\n",
            "AUCPR: 0.5841\n",
            "Threshold: 0.2693\n",
            "Confusion Matrix:\n",
            "[[86015 12888]\n",
            " [ 7992 13009]]\n",
            "\n",
            "Feature Importance:\n",
            "|     | Feature           |   Importance |\n",
            "|----:|:------------------|-------------:|\n",
            "|   7 | UrbanRural        |    0.0758303 |\n",
            "|   6 | FranchiseCode     |    0.0612762 |\n",
            "| 112 | BankState_VA      |    0.0521293 |\n",
            "|  68 | BankState_CA      |    0.0515097 |\n",
            "| 146 | SBAGuaranteeRatio |    0.0335086 |\n",
            "| 134 | RevLineCr_T       |    0.0310501 |\n",
            "| 135 | RevLineCr_Y       |    0.029907  |\n",
            "| 101 | BankState_OH      |    0.0297523 |\n",
            "|  93 | BankState_NC      |    0.0272023 |\n",
            "| 106 | BankState_RI      |    0.0228571 |\n",
            "|  38 | State_MT          |    0.0214886 |\n",
            "| 121 | RevLineCr_0       |    0.0208042 |\n",
            "|  80 | BankState_IL      |    0.0179449 |\n",
            "|  63 | Bank_freq         |    0.017307  |\n",
            "| 100 | BankState_NY      |    0.0163782 |\n",
            "Saved SHAP summary plot to shap_summary.png\n",
            "\n",
            "Scoring holdout data...\n",
            "   index  label  probability_0  probability_1\n",
            "0      0      0       0.925438       0.074562\n",
            "1      1      0       0.757611       0.242389\n",
            "2      2      0       0.936896       0.063104\n",
            "3      3      0       0.969148       0.030852\n",
            "4      4      0       0.904875       0.095125\n",
            "Predictions saved to holdout_predictions.csv\n",
            "\n",
            "Creating Kaggle submission...\n",
            "Kaggle submission saved to submission.csv\n",
            "   ID  probability_1\n",
            "0   0       0.074562\n",
            "1   1       0.242389\n",
            "2   2       0.063104\n",
            "3   3       0.030852\n",
            "4   4       0.095125\n",
            "Model saved to sba_loan_model.pkl\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve, confusion_matrix\n",
        "from scipy.stats import uniform, randint\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ======================\n",
        "# 1. DATA LOADING\n",
        "# ======================\n",
        "def load_data(filepath, chunksize=None):\n",
        "    \"\"\"Safe loading with chunking for large files\"\"\"\n",
        "    try:\n",
        "        if chunksize and ('csv' in filepath):\n",
        "            chunks = []\n",
        "            for chunk in pd.read_csv(filepath, chunksize=chunksize):\n",
        "                chunks.append(chunk)\n",
        "            return pd.concat(chunks, axis=0)\n",
        "        return pd.read_csv(filepath)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filepath}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "print(\"Loading data...\")\n",
        "data = load_data('SBA_loans_project_2.csv', chunksize=10000)\n",
        "holdout_data = load_data('SBA_loans_project_2_holdout_students_valid.csv')\n",
        "\n",
        "# ======================\n",
        "# 2. DATA CLEANING (Original Working Version)\n",
        "# ======================\n",
        "def clean_data(df):\n",
        "    \"\"\"Comprehensive cleaning pipeline\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    cols_to_drop = ['index', 'Unnamed: 0']\n",
        "    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
        "\n",
        "    # Clean monetary columns\n",
        "    money_cols = ['DisbursementGross', 'GrAppv', 'SBA_Appv']\n",
        "    for col in money_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = (df[col].astype(str)\n",
        "                       .str.replace(r'[^\\d.]', '', regex=True)\n",
        "                       .astype('float32'))\n",
        "\n",
        "    # Handle categorical missing values\n",
        "    cat_cols = ['RevLineCr', 'LowDoc']\n",
        "    for col in cat_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('MISSING').astype('category')\n",
        "\n",
        "    # Convert dates\n",
        "    date_cols = ['ApprovalDate', 'DisbursementDate']\n",
        "    for col in date_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Cleaning data...\")\n",
        "data = clean_data(data)\n",
        "holdout_data = clean_data(holdout_data)\n",
        "\n",
        "# ======================\n",
        "# 3. TARGET PROCESSING (Fixed Version)\n",
        "# ======================\n",
        "def prepare_target(df):\n",
        "    \"\"\"Fixed target processing handling all cases\"\"\"\n",
        "    if 'MIS_Status' not in df.columns:\n",
        "        raise ValueError(\"Target column 'MIS_Status' not found\")\n",
        "\n",
        "    # Convert to string first to ensure str accessor works\n",
        "    y_series = df['MIS_Status'].astype(str).str.upper()\n",
        "\n",
        "    # Handle all possible cases\n",
        "    mapping = {\n",
        "        'PIF': 0, '0': 0,\n",
        "        'PCHGB': 1, '1': 1,\n",
        "        'CHGOFF': 1,  # Additional variant seen in some datasets\n",
        "        'EXEMPT': np.nan, 'NAN': np.nan, '': np.nan\n",
        "    }\n",
        "\n",
        "    y = y_series.map(mapping).astype('float32')\n",
        "\n",
        "    # Check for unmapped values\n",
        "    if y.isna().any():\n",
        "        unmapped = y_series[y.isna()].unique()\n",
        "        print(f\"Warning: Unmapped values in target: {unmapped}\")\n",
        "        print(\"These will be dropped from the dataset\")\n",
        "        y = y.dropna()\n",
        "\n",
        "    return y.astype('int8'), df.loc[y.index]\n",
        "\n",
        "print(\"Preparing target...\")\n",
        "y, data = prepare_target(data)\n",
        "\n",
        "# ======================\n",
        "# 4. FEATURE ENCODING (UPDATED)\n",
        "# ======================\n",
        "def encode_features(train_df, test_df=None, cat_threshold=100):\n",
        "    \"\"\"Adaptive encoding with proper categorical handling\"\"\"\n",
        "    train_df = train_df.copy()\n",
        "    if test_df is not None:\n",
        "        test_df = test_df.copy()\n",
        "\n",
        "    # Identify categoricals (including string columns)\n",
        "    cat_cols = [col for col in train_df.select_dtypes(include=['category', 'object']).columns\n",
        "               if col != 'MIS_Status']\n",
        "    print(f\"Found {len(cat_cols)} categorical features\")\n",
        "\n",
        "    # Apply encoding\n",
        "    for col in cat_cols:\n",
        "        # For string columns, first convert to category\n",
        "        if train_df[col].dtype == 'object':\n",
        "            train_df[col] = train_df[col].astype('category')\n",
        "            if test_df is not None:\n",
        "                test_df[col] = test_df[col].astype('category')\n",
        "\n",
        "        # Frequency encoding for high-cardinality\n",
        "        if train_df[col].nunique() > cat_threshold:\n",
        "            freq_map = train_df[col].value_counts(normalize=True)\n",
        "            train_df[f\"{col}_freq\"] = train_df[col].map(freq_map)\n",
        "            if test_df is not None:\n",
        "                test_df[f\"{col}_freq\"] = test_df[col].map(freq_map).fillna(0)\n",
        "            train_df = train_df.drop(columns=[col])\n",
        "            if test_df is not None:\n",
        "                test_df = test_df.drop(columns=[col])\n",
        "        # One-hot for low-cardinality\n",
        "        else:\n",
        "            train_df = pd.get_dummies(train_df, columns=[col], prefix=col, drop_first=True)\n",
        "            if test_df is not None:\n",
        "                test_df = pd.get_dummies(test_df, columns=[col], prefix=col, drop_first=True)\n",
        "\n",
        "    # Align test features\n",
        "    if test_df is not None:\n",
        "        missing_cols = set(train_df.columns) - set(test_df.columns)\n",
        "        for col in missing_cols:\n",
        "            test_df[col] = 0\n",
        "        test_df = test_df[train_df.columns]\n",
        "        return train_df, test_df\n",
        "\n",
        "    return train_df\n",
        "\n",
        "print(\"Encoding features...\")\n",
        "X = encode_features(data.drop(columns=['MIS_Status']))\n",
        "holdout_encoded = encode_features(holdout_data)\n",
        "\n",
        "# ======================\n",
        "# 5. FEATURE ENGINEERING\n",
        "# ======================\n",
        "def create_features(df):\n",
        "    \"\"\"Create interaction features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Financial ratios\n",
        "    if all(col in df.columns for col in ['DisbursementGross', 'GrAppv']):\n",
        "        df['DisbursementRatio'] = df['DisbursementGross'] / df['GrAppv'].replace(0, 1e-6)\n",
        "\n",
        "    if all(col in df.columns for col in ['GrAppv', 'SBA_Appv']):\n",
        "        df['SBAGuaranteeRatio'] = df['SBA_Appv'] / df['GrAppv'].replace(0, 1e-6)\n",
        "\n",
        "    # Temporal features\n",
        "    if all(col in df.columns for col in ['ApprovalDate', 'DisbursementDate']):\n",
        "        df['ProcessingDays'] = (df['DisbursementDate'] - df['ApprovalDate']).dt.days\n",
        "\n",
        "    # Business features\n",
        "    if 'NoEmp' in df.columns:\n",
        "        df['LogEmployees'] = np.log1p(df['NoEmp'])\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Engineering features...\")\n",
        "X = create_features(X)\n",
        "holdout_encoded = create_features(holdout_encoded)\n",
        "\n",
        "# ======================\n",
        "# 6. TRAIN-TEST SPLIT\n",
        "# ======================\n",
        "print(\"Splitting data...\")\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# ======================\n",
        "# 7. MODEL TRAINING (UPDATED)\n",
        "# ======================\n",
        "def train_model(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"XGBoost training with categorical support\"\"\"\n",
        "    param_dist = {\n",
        "        'learning_rate': uniform(0.01, 0.3),\n",
        "        'max_depth': randint(3, 7),\n",
        "        'subsample': uniform(0.6, 0.4),\n",
        "        'colsample_bytree': uniform(0.6, 0.4),\n",
        "        'gamma': uniform(0, 0.3),\n",
        "        'reg_lambda': uniform(1, 100),\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        objective='binary:logistic',\n",
        "        eval_metric='aucpr',\n",
        "        early_stopping_rounds=20,\n",
        "        tree_method='hist',\n",
        "        enable_categorical=True,  # Enable categorical support\n",
        "        n_estimators=300\n",
        "    )\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        model,\n",
        "        param_dist,\n",
        "        n_iter=15,\n",
        "        scoring='average_precision',\n",
        "        cv=3,\n",
        "        n_jobs=1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Convert remaining categoricals to integer codes\n",
        "    for col in X_train.select_dtypes(include=['category']).columns:\n",
        "        X_train[col] = X_train[col].cat.codes\n",
        "        X_val[col] = X_val[col].cat.codes\n",
        "\n",
        "    search.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0)\n",
        "    return search.best_estimator_\n",
        "\n",
        "print(\"Training model...\")\n",
        "model = train_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "# ======================\n",
        "# 8. EVALUATION\n",
        "# ======================\n",
        "def evaluate(model, X, y):\n",
        "    \"\"\"Comprehensive evaluation\"\"\"\n",
        "    y_pred = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    # AUCPR\n",
        "    aucpr = average_precision_score(y, y_pred)\n",
        "    print(f\"AUCPR: {aucpr:.4f}\")\n",
        "\n",
        "    # Optimal threshold\n",
        "    precision, recall, thresholds = precision_recall_curve(y, y_pred)\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "    best_idx = np.argmax(f1_scores)\n",
        "    threshold = thresholds[best_idx]\n",
        "\n",
        "    # Confusion matrix\n",
        "    y_class = (y_pred >= threshold).astype(int)\n",
        "    cm = confusion_matrix(y, y_class)\n",
        "    print(f\"Threshold: {threshold:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    return aucpr, threshold\n",
        "\n",
        "print(\"\\nValidation Evaluation:\")\n",
        "val_aucpr, threshold = evaluate(model, X_val, y_val)\n",
        "print(\"\\nTest Evaluation:\")\n",
        "test_aucpr, _ = evaluate(model, X_test, y_test)\n",
        "\n",
        "# ======================\n",
        "# 9. INTERPRETATION\n",
        "# ======================\n",
        "def interpret(model, X_train, X_test, top_n=15):\n",
        "    \"\"\"Memory-efficient interpretation\"\"\"\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    importance = pd.DataFrame({\n",
        "        'Feature': X_train.columns,\n",
        "        'Importance': model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(importance.head(top_n).to_markdown())\n",
        "\n",
        "    # SHAP summary (sampled)\n",
        "    try:\n",
        "        import shap\n",
        "        sample = X_train.sample(n=min(1000, len(X_train)), random_state=42)\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(sample)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        shap.summary_plot(shap_values, sample, show=False)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('shap_summary.png')\n",
        "        plt.close()\n",
        "        print(\"Saved SHAP summary plot to shap_summary.png\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate SHAP: {str(e)}\")\n",
        "\n",
        "interpret(model, X_train, X_test)\n",
        "\n",
        "# ======================\n",
        "# 10. HOLDOUT SCORING (UPDATED)\n",
        "# ======================\n",
        "def predict_holdout(model, holdout, threshold):\n",
        "    \"\"\"Safe holdout prediction with correct output format\"\"\"\n",
        "    # Align columns\n",
        "    missing_cols = set(X_train.columns) - set(holdout.columns)\n",
        "    for col in missing_cols:\n",
        "        holdout[col] = 0\n",
        "    holdout = holdout[X_train.columns]\n",
        "\n",
        "    # Predict in chunks\n",
        "    chunks = []\n",
        "    for i in range(0, len(holdout), 5000):\n",
        "        chunk = holdout.iloc[i:i+5000]\n",
        "        proba = model.predict_proba(chunk)\n",
        "        chunks.append(pd.DataFrame({\n",
        "            'index': chunk.index,\n",
        "            'label': (proba[:, 1] >= threshold).astype(int),\n",
        "            'probability_0': proba[:, 0],\n",
        "            'probability_1': proba[:, 1]\n",
        "        }))\n",
        "\n",
        "    # Combine chunks and ensure correct column order\n",
        "    result = pd.concat(chunks)\n",
        "    result = result[['index', 'label', 'probability_0', 'probability_1']]\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"\\nScoring holdout data...\")\n",
        "holdout_preds = predict_holdout(model, holdout_encoded, threshold)\n",
        "print(holdout_preds.head())\n",
        "\n",
        "# Save predictions\n",
        "holdout_preds.to_csv('holdout_predictions.csv', index=False)\n",
        "print(\"Predictions saved to holdout_predictions.csv\")\n",
        "\n",
        "# ======================\n",
        "# KAGGLE SUBMISSION FORMAT\n",
        "# ======================\n",
        "def create_kaggle_submission(model, holdout, filename='submission.csv'):\n",
        "    \"\"\"Create submission file in Kaggle format\"\"\"\n",
        "    # Align columns\n",
        "    missing_cols = set(X_train.columns) - set(holdout.columns)\n",
        "    for col in missing_cols:\n",
        "        holdout[col] = 0\n",
        "    holdout = holdout[X_train.columns]\n",
        "\n",
        "    # Predict in chunks\n",
        "    chunks = []\n",
        "    for i in range(0, len(holdout), 5000):\n",
        "        chunk = holdout.iloc[i:i+5000]\n",
        "        proba = model.predict_proba(chunk)[:, 1]  # Only probability_1\n",
        "        chunks.append(pd.DataFrame({\n",
        "            'ID': chunk.index,\n",
        "            'probability_1': proba\n",
        "        }))\n",
        "\n",
        "    # Combine and save\n",
        "    submission = pd.concat(chunks)\n",
        "    submission.to_csv(filename, index=False)\n",
        "    print(f\"Kaggle submission saved to {filename}\")\n",
        "    return submission\n",
        "\n",
        "# Create Kaggle submission file\n",
        "print(\"\\nCreating Kaggle submission...\")\n",
        "kaggle_submission = create_kaggle_submission(model, holdout_encoded)\n",
        "print(kaggle_submission.head())\n",
        "\n",
        "# Save model\n",
        "import joblib\n",
        "joblib.dump(model, 'sba_loan_model.pkl')\n",
        "print(\"Model saved to sba_loan_model.pkl\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONkbCDvYr4KJM/WWy1FaOe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}